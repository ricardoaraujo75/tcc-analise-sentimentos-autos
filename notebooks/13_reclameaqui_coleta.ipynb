{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Coleta de Dados do Reclame Aqui\n",
    "\n",
    "Este notebook apresenta duas formas de coletar dados do Reclame Aqui para o TCC:\n",
    "\n",
    "- **Opção A**: Scraping automático usando `requests` e `BeautifulSoup`.\n",
    "- **Opção B**: Importação de um CSV manual, exportado previamente pelo pesquisador.\n",
    "\n",
    "A saída final será um DataFrame padronizado com as colunas:\n",
    "\n",
    "`data, marca, modelo, texto, status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências (caso necessário)\n",
    "!pip install requests beautifulsoup4 pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opção A – Scraping Automático\n",
    "\n",
    "⚠️ **Aviso**: O site Reclame Aqui pode bloquear acessos automatizados. Use esta abordagem apenas para fins acadêmicos, em volume limitado.\n",
    "\n",
    "Aqui um exemplo simples para coletar títulos de reclamações de uma página da Hyundai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.reclameaqui.com.br/empresa/hyundai-motor-brasil/\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Exemplo: pegar títulos de reclamações (simples)\n",
    "titulos = [t.get_text(strip=True) for t in soup.find_all(\"h4\")]\n",
    "\n",
    "df_scraping = pd.DataFrame({\n",
    "    \"data\": [datetime.today().strftime(\"%Y-%m-%d\")] * len(titulos),\n",
    "    \"marca\": [\"Hyundai\"] * len(titulos),\n",
    "    \"modelo\": [\"HB20\"] * len(titulos),\n",
    "    \"texto\": titulos,\n",
    "    \"status\": [\"Desconhecido\"] * len(titulos)\n",
    "})\n",
    "\n",
    "df_scraping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opção B – Importação de CSV Manual\n",
    "\n",
    "Se preferir exportar manualmente as reclamações (copiando para uma planilha e salvando em CSV), basta usar este trecho de código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitua pelo caminho do seu CSV\n",
    "csv_path = \"reclameaqui_carros.csv\"\n",
    "\n",
    "try:\n",
    "    df_manual = pd.read_csv(csv_path)\n",
    "    print(\"Pré-visualização dos dados carregados:\")\n",
    "    display(df_manual.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Arquivo CSV não encontrado. Salve o arquivo e atualize o caminho em 'csv_path'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padronização Final\n",
    "\n",
    "Independente da opção escolhida, padronizamos o DataFrame final com as colunas necessárias:\n",
    "- `data`\n",
    "- `marca`\n",
    "- `modelo`\n",
    "- `texto`\n",
    "- `status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se veio do scraping\n",
    "if 'df_scraping' in locals():\n",
    "    df_final = df_scraping.copy()\n",
    "elif 'df_manual' in locals():\n",
    "    df_final = df_manual.copy()\n",
    "else:\n",
    "    df_final = pd.DataFrame(columns=[\"data\", \"marca\", \"modelo\", \"texto\", \"status\"])\n",
    "\n",
    "print(\"DataFrame padronizado:\")\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos passos\n",
    "\n",
    "- Expandir scraping para coletar mais detalhes (data real da reclamação, status resolvido/não resolvido, etc.).\n",
    "- Usar CSV manual para complementar caso o scraping seja limitado.\n",
    "- Integrar este DataFrame ao pipeline de pré-processamento de sentimentos.\n",
    "- Armazenar no banco de dados (PostgreSQL) junto com os demais datasets (TweetEval e Twitter)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
